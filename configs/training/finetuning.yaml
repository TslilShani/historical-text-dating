# Training configuration for finetuning
max_epochs: 10
batch_size: 8
learning_rate: 2e-5
betas: [0.9, 0.999]
grad_norm_clip: 1.0
weight_decay: 0.01
num_workers: 4
lr_decay: true
warmup_steps: 100
scheduler:
  scheduler_type: "LinearLR"
  start_factor: 1.0
  end_factor: 0.01
seed: 1234
ckpt_path: ./outputs/${model.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}/
