# Training configuration for finetuning
max_epochs: 40
batch_size: 8
learning_rate: 2e-4
betas: [0.9, 0.999]
grad_norm_clip: 1.0
weight_decay: 0.01
warmup_ratio: 0.1
num_workers: 4
lr_decay: true
warmup_tokens: 1000
final_tokens: 10000
seed: 1234
ckpt_path: ./outputs/${model.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}/
