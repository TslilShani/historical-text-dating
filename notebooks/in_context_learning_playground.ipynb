{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from src.models.model_head import HistoricalTextDatingModel, create_model_head_config\n",
    "from src.utils import init_tracker, DataLoadAndFilter\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../configs/defaults.yaml\"\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config path does not exist: {config_path}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"defaults\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Error loading schema ../data/raw/SefariaData/Sefaria-Export-master/schemas/Sheet.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loading Sefaria texts: 100%|██████████| 6549/6549 [00:14<00:00, 460.24it/s] \n",
      "Loading Ben Yehuda texts: 21208it [00:03, 5604.92it/s]\n",
      "Loading Royal Society texts: 17520it [00:02, 7871.31it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "encoder = transformers.AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n",
    "# Load datasets using the data loader\n",
    "data_loader = DataLoadAndFilter(cfg)\n",
    "train_dataset, eval_dataset = data_loader.load_datasets(base_path=\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_at_k(preds, labels, K=1):\n",
    "    \"\"\"\n",
    "    Flexible Acc@K: correct if prediction within ±floor(K/2) of true class.\n",
    "    preds: list of predicted indices (as int)\n",
    "    labels: list of true indices (as int)\n",
    "    K: window size\n",
    "    \"\"\"\n",
    "    allowed_distance = K // 2\n",
    "    preds = np.asarray(preds)\n",
    "    labels = np.asarray(labels)\n",
    "    correct = np.abs(preds - labels) <= allowed_distance\n",
    "    return float(np.mean(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates_by_mask_likelihood(sentence_with_mask, candidates, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Given a sentence with one mask token and a list of candidate words,\n",
    "    returns the candidates sorted by model likelihood for the mask position.\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(sentence_with_mask, return_tensors=\"pt\")\n",
    "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, mask_token_index]\n",
    "    # Get scores for each candidate\n",
    "    candidate_scores = []\n",
    "    for word in candidates:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)[0])\n",
    "        score = logits[token_id].item()\n",
    "        candidate_scores.append((word, score))\n",
    "    # Sort by score descending\n",
    "    return sorted(candidate_scores, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2000', 4.284444332122803), ('1900', 3.8327479362487793), ('1500', 3.5688230991363525), ('1800', 2.058945655822754), ('2025', 0.636199414730072)]\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"This text was written in {tokenizer.mask_token}.\"\n",
    "candidates = [\"1800\", \"1900\", \"2000\", \"1500\", \"2025\"]\n",
    "ranked = rank_candidates_by_mask_likelihood(sentence, candidates, tokenizer, encoder)\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batched(iterable, n):\n",
    "    \"\"\"Yield batches of n items from iterable.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(itertools.islice(it, n))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3904it [08:49,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 269/3903 = 6.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'acc_at_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     total_count += \u001b[32m1\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_count/total_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAcc@1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43macc_at_k\u001b[49m(top_preds,\u001b[38;5;250m \u001b[39mtrue_labels,\u001b[38;5;250m \u001b[39mK=\u001b[32m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAcc@3: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_at_k(top_preds,\u001b[38;5;250m \u001b[39mtrue_labels,\u001b[38;5;250m \u001b[39mK=\u001b[32m3\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAcc@5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_at_k(top_preds,\u001b[38;5;250m \u001b[39mtrue_labels,\u001b[38;5;250m \u001b[39mK=\u001b[32m5\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'acc_at_k' is not defined"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "top_preds = []\n",
    "true_labels = []\n",
    "\n",
    "candidates = list(map(str, data_loader.unique_date_ranges))\n",
    "question = \"This text was written in year\"\n",
    "seperator = \"\\n\"\n",
    "question_format = \"{text}. {question} {year}.\"\n",
    "\n",
    "for items in tqdm(batched(train_dataset, 3)):\n",
    "    if len(items) < 3:\n",
    "        continue\n",
    "    examples = items[:2]\n",
    "    test = items[2]\n",
    "    exapmle_strings = []\n",
    "    for item in examples:\n",
    "        text = item['text']\n",
    "        year = item['comp_date']\n",
    "        text = text[:500 // 3]\n",
    "        sentence = question_format.format(text=text, question=question, year=year)\n",
    "        exapmle_strings.append(sentence)\n",
    "    exapmle_strings.append(question_format.format(text=test['text'][:500 // 3], question=question, year=tokenizer.mask_token))\n",
    "    ranked = rank_candidates_by_mask_likelihood(seperator.join(exapmle_strings), candidates, tokenizer, encoder)\n",
    "    top_choice = ranked[0][0]\n",
    "    top_preds.append(candidates.index(top_choice))\n",
    "    true_labels.append(candidates.index(str(test['comp_date'])))\n",
    "    if top_choice == str(test['comp_date']):\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "print(f\"Accuracy: {correct_count}/{total_count} = {correct_count/total_count:.2%}\")\n",
    "print(f\"Acc@1: {acc_at_k(top_preds, true_labels, K=1):.2%}\")\n",
    "print(f\"Acc@3: {acc_at_k(top_preds, true_labels, K=3):.2%}\")\n",
    "print(f\"Acc@5: {acc_at_k(top_preds, true_labels, K=5):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 269/3903 = 6.89%\n",
      "Acc@1: 6.89%\n",
      "Acc@3: 13.96%\n",
      "Acc@5: 18.86%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Accuracy: {correct_count}/{total_count} = {correct_count/total_count:.2%}\")\n",
    "print(f\"Acc@1: {acc_at_k(top_preds, true_labels, K=1):.2%}\")\n",
    "print(f\"Acc@3: {acc_at_k(top_preds, true_labels, K=3):.2%}\")\n",
    "print(f\"Acc@5: {acc_at_k(top_preds, true_labels, K=5):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over train dataset:   \n",
    "Accuracy: 3298/3903 = 84.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8867924528301887"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 / (len(data_loader.unique_date_ranges) // 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
