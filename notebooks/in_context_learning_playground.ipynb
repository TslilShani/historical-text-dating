{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from src.models.model_head import HistoricalTextDatingModel, create_model_head_config\n",
    "from src.utils import init_tracker, DataLoadAndFilter\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../configs/defaults.yaml\"\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config path does not exist: {config_path}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"defaults\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Error loading schema ../data/raw/SefariaData/Sefaria-Export-master/schemas/Sheet.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loading Sefaria texts: 100%|██████████| 6549/6549 [00:14<00:00, 460.24it/s] \n",
      "Loading Ben Yehuda texts: 21208it [00:03, 5604.92it/s]\n",
      "Loading Royal Society texts: 17520it [00:02, 7871.31it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "encoder = transformers.AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n",
    "# Load datasets using the data loader\n",
    "data_loader = DataLoadAndFilter(cfg)\n",
    "train_dataset, eval_dataset = data_loader.load_datasets(base_path=\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates_by_mask_likelihood(sentence_with_mask, candidates, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Given a sentence with one mask token and a list of candidate words,\n",
    "    returns the candidates sorted by model likelihood for the mask position.\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(sentence_with_mask, return_tensors=\"pt\")\n",
    "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, mask_token_index]\n",
    "    # Get scores for each candidate\n",
    "    candidate_scores = []\n",
    "    for word in candidates:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)[0])\n",
    "        score = logits[token_id].item()\n",
    "        candidate_scores.append((word, score))\n",
    "    # Sort by score descending\n",
    "    return sorted(candidate_scores, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2000', 4.284444332122803), ('1900', 3.8327479362487793), ('1500', 3.5688230991363525), ('1800', 2.058945655822754), ('2025', 0.636199414730072)]\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"This text was written in {tokenizer.mask_token}.\"\n",
    "candidates = [\"1800\", \"1900\", \"2000\", \"1500\", \"2025\"]\n",
    "ranked = rank_candidates_by_mask_likelihood(sentence, candidates, tokenizer, encoder)\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batched(iterable, n):\n",
    "    \"\"\"Yield batches of n items from iterable.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(itertools.islice(it, n))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3904it [06:13, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 3298/3903 = 84.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "candidates = list(map(str, data_loader.unique_date_ranges))\n",
    "question = f\"This text was written in year\"\n",
    "seperator = \"\\n\"\n",
    "question_format = \"{text}. {question} {year}.\"\n",
    "for items in tqdm(batched(train_dataset, 3)):\n",
    "    if len(items) < 3:\n",
    "        continue\n",
    "    examples = items[:2]\n",
    "    test = items[2]\n",
    "    test['text'] = test['text'][:500 // 3]  # Truncate to fit model input size\n",
    "    exapmle_strings = []\n",
    "    for item in examples:\n",
    "        text = item['text']\n",
    "        year = item['comp_date']\n",
    "        text = text[:500 // 3]  # Truncate to fit model input size\n",
    "        sentence = question_format.format(text=text, question=question, year=year)\n",
    "        exapmle_strings.append(sentence)\n",
    "    exapmle_strings.append(question_format.format(text=test['text'], question=question, year=tokenizer.mask_token))\n",
    "    ranked = rank_candidates_by_mask_likelihood(seperator.join(exapmle_strings), candidates, tokenizer, encoder)\n",
    "    top_choice = ranked[0][0]\n",
    "    if top_choice == str(year):\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "print(f\"Accuracy: {correct_count}/{total_count} = {correct_count/total_count:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over train dataset:   \n",
    "Accuracy: 3298/3903 = 84.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8867924528301887"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 / (len(data_loader.unique_date_ranges) // 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
