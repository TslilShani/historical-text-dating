{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yonatan/.conda/envs/nlp_final_project/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from src.models.model_head import HistoricalTextDatingModel, create_model_head_config\n",
    "from src.utils import init_tracker, DataLoadAndFilter\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Set your checkpoint path here\n",
    "ckpt_base_path = \"../outputs/google-bert/bert-base-multilingual-cased/2025-09-15/23-08-03/\"\n",
    "config_path = ckpt_base_path + \".hydra/config.yaml\"\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config path does not exist: {config_path}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Error loading schema ../data/raw/SefariaData/Sefaria-Export-master/schemas/Sheet.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loading Sefaria texts: 100%|██████████| 100/100 [00:00<00:00, 358.10it/s]\n",
      "Loading Ben Yehuda texts: 100%|██████████| 100/100 [00:00<00:00, 12499.79it/s]\n",
      "Loading Royal Society texts: 100%|██████████| 100/100 [00:00<00:00, 25852.47it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = instantiate(cfg.model.tokenizer)\n",
    "encoder = transformers.AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n",
    "# Load datasets using the data loader\n",
    "data_loader = DataLoadAndFilter(cfg)\n",
    "train_dataset, eval_dataset = data_loader.load_datasets(base_path=\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "First we try a naive approch to complete a scentence with one token to evaluate one-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_word(text: str, tokenizer, model, k=5):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Find all mask token positions\n",
    "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    # For each mask token, get top-k predictions\n",
    "    results = []\n",
    "    for idx in mask_token_indices:\n",
    "        mask_token_logits = predictions[0, idx, :]\n",
    "        top_k_tokens = torch.topk(mask_token_logits, k, dim=0).indices.tolist()\n",
    "        decoded = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n",
    "        print(f\"Mask at position {idx.item()}: {decoded}\")\n",
    "        results.append(decoded)\n",
    "    print(\"\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask at position 8: ['I', '1', '.', '2', '5']\n",
      "Mask at position 10: ['.', ':', 'the', '-', 'a']\n",
      "\n",
      "\n",
      "Mask at position 7: ['Israel', '1948', '1', '6', '9']\n",
      "Mask at position 9: ['Israel', '1948', '1947', '1946', '1967']\n",
      "\n",
      "\n",
      "Mask at position 11: ['2', '1', 'independence', '1918', '3']\n",
      "Mask at position 13: ['1918', '1910', 'independence', '1919', '1914']\n",
      "\n",
      "\n",
      "Mask at position 7: [':', '.', 'Obama', 'Karachi', 'Kabul']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_masked_word(f\"This text was written between the years {tokenizer.mask_token} to {tokenizer.mask_token}\", tokenizer, encoder)\n",
    "predict_masked_word(f\"Israel declared independence between the years {tokenizer.mask_token} and {tokenizer.mask_token}\", tokenizer, encoder)\n",
    "predict_masked_word(f\"The United States Of America declared independence between the years {tokenizer.mask_token} and {tokenizer.mask_token}\", tokenizer, encoder)\n",
    "predict_masked_word(f\"Barak Obama was born at {tokenizer.mask_token}\", tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well the results are bad (except of the Israel report to our suprise).   \n",
    "Let's try to rank the possible dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates_by_mask_likelihood(sentence_with_mask, candidates, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Given a sentence with one mask token and a list of candidate words,\n",
    "    returns the candidates sorted by model likelihood for the mask position.\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(sentence_with_mask, return_tensors=\"pt\")\n",
    "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, mask_token_index]\n",
    "    # Get scores for each candidate\n",
    "    candidate_scores = []\n",
    "    for word in candidates:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)[0])\n",
    "        score = logits[token_id].item()\n",
    "        candidate_scores.append((word, score))\n",
    "    # Sort by score descending\n",
    "    return sorted(candidate_scores, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2000', 4.284444332122803), ('1900', 3.8327479362487793), ('1500', 3.5688230991363525), ('1800', 2.058945655822754), ('2025', 0.636199414730072)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = f\"This text was written in {tokenizer.mask_token}.\"\n",
    "candidates = [\"1800\", \"1900\", \"2000\", \"1500\", \"2025\"]\n",
    "ranked = rank_candidates_by_mask_likelihood(sentence, candidates, tokenizer, encoder)\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
