{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from src.models.model_head import HistoricalTextDatingModel, create_model_head_config\n",
    "from src.utils import init_tracker, DataLoadAndFilter\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../configs/defaults.yaml\"\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config path does not exist: {config_path}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"defaults\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Error loading schema ../data/raw/SefariaData/Sefaria-Export-master/schemas/Sheet.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loading Sefaria texts: 100%|██████████| 6549/6549 [00:13<00:00, 468.26it/s] \n",
      "Loading Ben Yehuda texts: 21208it [00:03, 6130.06it/s]\n",
      "Loading Royal Society texts: 17520it [00:02, 8586.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "encoder = transformers.AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n",
    "# Load datasets using the data loader\n",
    "data_loader = DataLoadAndFilter(cfg)\n",
    "train_dataset, eval_dataset = data_loader.load_datasets(base_path=\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "First we try a naive approch to complete a scentence with one token to evaluate one-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_word(text: str, tokenizer, model, k=5):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Find all mask token positions\n",
    "    mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    # For each mask token, get top-k predictions\n",
    "    results = []\n",
    "    for idx in mask_token_indices:\n",
    "        mask_token_logits = predictions[0, idx, :]\n",
    "        top_k_tokens = torch.topk(mask_token_logits, k, dim=0).indices.tolist()\n",
    "        decoded = [tokenizer.decode([token_id]).strip() for token_id in top_k_tokens]\n",
    "        print(f\"Mask at position {idx.item()}: {decoded}\")\n",
    "        results.append(decoded)\n",
    "    print(\"\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask at position 8: ['I', '1', '.', '2', '5']\n",
      "Mask at position 10: ['.', ':', 'the', '-', 'a']\n",
      "\n",
      "\n",
      "Mask at position 7: ['Israel', '1948', '1', '6', '9']\n",
      "Mask at position 9: ['Israel', '1948', '1947', '1946', '1967']\n",
      "\n",
      "\n",
      "Mask at position 11: ['2', '1', 'independence', '1918', '3']\n",
      "Mask at position 13: ['1918', '1910', 'independence', '1919', '1914']\n",
      "\n",
      "\n",
      "Mask at position 7: [':', '.', 'Obama', 'Karachi', 'Kabul']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_masked_word(f\"This text was written between the years {tokenizer.mask_token} to {tokenizer.mask_token}\", tokenizer, encoder)\n",
    "predict_masked_word(f\"Israel declared independence between the years {tokenizer.mask_token} and {tokenizer.mask_token}\", tokenizer, encoder)\n",
    "predict_masked_word(f\"The United States Of America declared independence between the years {tokenizer.mask_token} and {tokenizer.mask_token}\", tokenizer, encoder)\n",
    "predict_masked_word(f\"Barak Obama was born at {tokenizer.mask_token}\", tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well the results are bad (except of the Israel report to our suprise).   \n",
    "Let's try to rank the possible dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_at_k(preds, labels, K=1):\n",
    "    \"\"\"\n",
    "    Flexible Acc@K: correct if prediction within ±floor(K/2) of true class.\n",
    "    preds: list of predicted indices (as int)\n",
    "    labels: list of true indices (as int)\n",
    "    K: window size\n",
    "    \"\"\"\n",
    "    allowed_distance = K // 2\n",
    "    preds = np.asarray(preds)\n",
    "    labels = np.asarray(labels)\n",
    "    correct = np.abs(preds - labels) <= allowed_distance\n",
    "    return float(np.mean(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates_by_mask_likelihood(sentence_with_mask, candidates, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Given a sentence with one mask token and a list of candidate words,\n",
    "    returns the candidates sorted by model likelihood for the mask position.\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(sentence_with_mask, return_tensors=\"pt\")\n",
    "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, mask_token_index]\n",
    "    # Get scores for each candidate\n",
    "    candidate_scores = []\n",
    "    for word in candidates:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)[0])\n",
    "        score = logits[token_id].item()\n",
    "        candidate_scores.append((word, score))\n",
    "    # Sort by score descending\n",
    "    return sorted(candidate_scores, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2000', 4.284444332122803), ('1900', 3.8327479362487793), ('1500', 3.5688230991363525), ('1800', 2.058945655822754), ('2025', 0.636199414730072)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = f\"This text was written in {tokenizer.mask_token}.\"\n",
    "candidates = [\"1800\", \"1900\", \"2000\", \"1500\", \"2025\"]\n",
    "ranked = rank_candidates_by_mask_likelihood(sentence, candidates, tokenizer, encoder)\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11710/11710 [26:14<00:00,  7.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 511/11710 = 4.36%\n",
      "Acc@1: 4.36%\n",
      "Acc@3: 9.72%\n",
      "Acc@5: 10.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = 0\n",
    "top_preds = []\n",
    "true_labels = []\n",
    "\n",
    "for item in tqdm(train_dataset):\n",
    "    text = item['text']\n",
    "    year = item['comp_date']\n",
    "    text = text[:500]  # Truncate to fit model input size\n",
    "    sentence = f\"{text}. This text was written in year {tokenizer.mask_token}.\"\n",
    "    candidates = list(map(str, data_loader.unique_date_ranges))\n",
    "    ranked = rank_candidates_by_mask_likelihood(sentence, candidates, tokenizer, encoder)\n",
    "    top_choice = ranked[0][0]\n",
    "    top_preds.append(candidates.index(top_choice))\n",
    "    true_labels.append(candidates.index(str(year)))\n",
    "    if top_choice == str(year):\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "print(f\"Accuracy: {correct_count}/{total_count} = {correct_count/total_count:.2%}\")\n",
    "print(f\"Acc@1: {acc_at_k(top_preds, true_labels, K=1):.2%}\")\n",
    "print(f\"Acc@3: {acc_at_k(top_preds, true_labels, K=3):.2%}\")\n",
    "print(f\"Acc@5: {acc_at_k(top_preds, true_labels, K=5):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over train dataset:   \n",
    "Accuracy: 511/11710 = 4.36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 / len(data_loader.unique_date_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
